\documentclass[10pt,twoside,slovak,a4paper]{article}
\usepackage[slovak]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{url}

\pagestyle{headings}

\title{Paralelný web crawler\thanks{Semestrálny projekt v predmete Metódy inžinierskej práce, ak. rok 2023/24, vedenie: Ing. Richard Marko, PhD.}}

\author{Peter Kapusta\\[2pt]
	{\small Slovenská technická univerzita v Bratislave}\\
	{\small Fakulta informatiky a informačných technológií}\\
	{\small \texttt{xkapustap@stuba.sk}}}

\date{\small 30. september 2023} % Zmeniť

\begin{document}

\maketitle

\begin{abstract}
\ldots
\end{abstract}

\section{Úvod}

Vo všeobecnosti, web crawler je program, ktorý skenuje web, sťahuje webstránky a následne z nich extrahuje informácie, ktoré môže neskôr využiť napr. prehliadač. Cieľom je teda čo najefektívnejšie a najrýchlejšie objaviť čo najväčšie množstvo informácií. Hlavnou ťažkosťou je prehľadávanie HTML kódu a získavanie využiteľných odkazov, ktoré vedú k novým webstránkam. Čo ale robí paralelizované web crawlery špeciálnymi, je ich schopnosť súčasne sťahovať webstránky z rôznych zdrojov, bez toho, aby sa osobitné procesy prekrývali, aby sa čo najoptimálnejšie využila maximálna rýchlosť sťahovania. V článku spracujem a vysvetlím využitie paralelných web crawlerov, problematiku komunikácie medzi jednotlivými procesmi a uvediem riešenia, ktoré sa momentálne využívajú.

%\bibliography{literatura}
%\bibliographystyle{plain}

\end{document}
